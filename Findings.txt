PC specs:
Intel Core i7-2630QM CPU @ 2.00Ghz (Quadcore)
Ram: 7.7 GB
Architektur: Ubuntu 11.10 @ x86_64


Erster Versuch von Import:
Documentenstruktur:
  model: "Europa",
  scenario: "GRAS",
  year: 2001
  month: 1,
  data:
    pre: [[]],
    tmp: [[]]

Für jede Zeile in den alarm-txts prüfen ob das document schon in der db ist und das data array manipulieren
=> Extreeeeeem langsam

2. Import versuch, selbe db struktur. Es wird jeweils ein scenarion eingelesen und die kompletten Daten werden
in einem Hash gehalten und am Schluss wird aus dem Hash eine .json Datei erstellt, die alle DB Documente
enthält. Diese wird mint Mongoimport dann in die db eingelesen. Arbeitsspeicherverbrauch beim erstellen der 
JSON Datei 4,8 GB.
=> Import recht schnell (ca ~2h für ein Szenario)

Abfrage für den Durschnitt eines Jahres mit map/reduce und Mongo:
=> Problem, da map/reduce in Mongo kein Live-Query ist, sondern nur eine weitere Collection mit den Ergebnissen
des Map Reduces erstellt. Nach einer Änderung der Daten müsste also das Map/Reduce komplett neu durchgeführt werden.
=> Dauert bei einem Szenario 93 sekunden. (Allerdings für alle 100 Jahre)
Abfrage:
  db.climas.mapReduce(function() {
    if(this.data.pre) {
      emit(1,{pre: this.data.pre}); 
    }
  
  }, function(key, reduceArr) { 
    var result = { pre: reduceArr[0].pre }; 
    for(var i = 1;i<reduceArr.length;i++) {
      values = reduceArr[i].pre
      for(var x = 0;x<values.length;x++) {
        if(!values[x]) { continue; }
        for(var y = 0;y<values[x].length;y++) {
            result.pre[x][y] = (result.pre[x][y] + values[x][y]) / 2; <-- nicht korrekter AVG (für die laufzeit aber egal)
        }
      }
    }
  
    return result;
  },{out: {inline: 1}})

2. Versuch der Abfrage, diesmal mit ChouchDb (weil Couch map/reduce zwischenergebnisse cached damit spätere schneller berechnet werden).
Import Kommando für couchdb: siehe shell script in db/tmp/
Design Document:
  {
     "_id": "_design/alarm",
     "views": {
         "monthly_avg": {
             "map": "...",
             "reduce": "..."
         }
     }
  }
  
######## Map Function: 
function(doc) {
  if(doc.data)emit([doc.model,doc.scenario,doc.year],doc.data.pre);
}
  
######## Reduce Function:
function(keys, values, rereduce){
  var result = [];
    
  // Alle Nodes zusammenzählen
  function add(i) {
    for(var x=0;x<values[i].length;x++) {
      if(!values[i][x]) { continue; }
      for(var y=0;y<values[i][x].length;y++) {
        if(!values[i][x][y]) {continue;}
        result[x][y] = result[x][y] + values[i][x][y];
      }
    }
  }

  result = values[0];
  for(var i=1;i<values.length;i++) {
    add(i);
  }

  // Alle Values durch die Anzahl an zusammengezählten Nodes teilen => Average
  if(values.length > 1) {
    for(var x=0;x<result.length;x++) {
      if(!result[x]) { continue; }
      for(var y=0;y<result[x].length;y++) {
        if(!result[x][y]) {continue;}
        result[x][y] = result[x][y] / values.length;
      }
    }
  }
  

  return result;
}
  
Query: http://127.0.0.1:5984/alarm/_design/alarm/_view/monthly_avg?start_key=[2001],end_key=[2002]

Ergebnis: Der Aufbau des Indexes dauert mehrere Stunden, die Abfragen an sich sind dann auch nicht wirklich schnell.
Ursache: Damit diese Reduce Funktion überhaupt geht, musste in der Config von CouchDB das Reduce-Limit deaktiviert werden. 
  Vermutlich verträgt CouchDb keine so großen Werte als Reduce-Rückgabe, so dass die Performance deshalb extrem in die Knie geht.
Fazit: MongoDb mach den selben Map/Reduce, für den Couch Stunden braucht, in ~93 sekunden => Viel Performanter


Endergebnis mittels MongoDB und obiger Datenbankstruktur:
#############
Ergebnis für die mapval Funktion, die die Differenz zwischen zwei Jahren bildet:

Started GET "/mapval/Europe/GRAS/2001/Avg/all.json" for 127.0.0.1 at 2012-01-08 16:10:59 +0100
Completed 200 OK in 4886ms (Views: 909.0ms)

Started GET "/mapval/Europe/GRAS/2001/Avg/tmp.json" for 127.0.0.1 at 2012-01-08 16:16:02 +0100
Completed 200 OK in 1711ms (Views: 290.8ms)

#############
Ergebnis für die mapdiff Funktion, die die Differenz zwischen zwei Jahren bildet:

Started GET "/mapdiff/Europe/GRAS/2001/Avg/2002/Avg/all.json" for 127.0.0.1 at 2012-01-08 16:16:47 +0100
Completed 200 OK in 9140ms (Views: 941.1ms)

Started GET "/mapdiff/Europe/GRAS/2001/Avg/2002/Avg/tmp.json" for 127.0.0.1 at 2012-01-08 16:17:10 +0100
Completed 200 OK in 2947ms (Views: 301.1ms)

#############
Ergebnis für die proval Funktion, die das absolute Minimum, Maximum und Durchschnitt findet:
Propval.build "all", { :scenario => "GRAS" }
=> {"results"=>[{"_id"=>1.0, "value"=>{"pre"=>{"min"=>0.1, "max"=>990.8, "avg"=>63.27447506239002}, "tmp"=>{"min"=>0.1, "max"=>45.3, "avg"=>11.988376757463943}, "gdd"=>{"min"=>1.0, "max"=>1249.0, "avg"=>236.22401287232884}}}], "timeMillis"=>247760, "counts"=>{"input"=>1200, "emit"=>1200, "reduce"=>2, "output"=>1}, "ok"=>1.0}